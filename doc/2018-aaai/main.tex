\documentclass[]{article}

%opening
\title{Enhancing Coherence for Neural Extractive Summarization with Reinforcement Learning}  % may include Adversarial Training (or named as CoherenceGAN) as well
\author{Jimmy Wu}

\begin{document}

\maketitle

\begin{abstract}
\textbf{Work in progress}\\
Problem setting: neural extractive summarization \\
Research gap: coherence under this setting received little attention \\
Contribution: an end-to-end model for enhancing coherence \\
Evaluation and conclusion: WIP \\

\end{abstract}

\section{Introduction}


\section{Related Works}
Limitations of previous extractive summarization models using Deep Learning:
\begin{enumerate}
	\item They do not take coherence into consideration.
\end{enumerate}


Drawbacks of previous methods that tries to enhance coherence for summaries:
\begin{enumerate}
	\item Previous methods are developed based on \textbf{hand-crafted low level features} such as entity-grid or word co-occurrence.
	\item Automatic evaluation of coherence is based on linguistic theorems such as Centering theorem and other similarity discovered by linguistics, which are in turn based on the observation of what is coherent text and what is not. This process is \textbf{not data-driven} and is doomed to be based by human observation.
	\item The introduction of coherence is often through post-processing. It does not directly effect the probability that a sentence is chosen? (this is doubtful)
\end{enumerate}


\section{Model}



\end{document}
